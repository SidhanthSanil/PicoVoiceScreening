{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) [C, Python] The probability of rain on a given calendar day in Vancouver is p[i], where i is the day's index. For\n",
    "example, p[0] is the probability of rain on January 1st, and p[10] is the probability of precipitation on January 11th.\n",
    "\n",
    "Assume the year has 365 days (i.e., p has 365 elements). What is the chance it rains more than n (e.g., 100) days in Vancouver?\n",
    "Write a function that accepts p (probabilities of rain on a given calendar day) and n as input arguments and returns the\n",
    "possibility of raining at least n days.\n",
    "\n",
    "\n",
    "def prob_rain_more_than_n(p: Sequence[float], n: int) -> float:\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption made: Using the Monte Carlo simulation for this problem as it handles the complexity of varying daily rain probabilities well. Each day has its own chance of rain, so using straightforward analytical methods like the binomial distribution isn’t an ideal approach.\n",
    "\n",
    "Reasons:\n",
    "Firstly, Monte Carlo simulation is great because it simulates the entire year thousands of times, and shows how often we end up with more than a certain number of rainy days. \n",
    "Secondly, it's flexible, in order to tweak the probabilities or the number of days, I can do that without reworking the whole approach. \n",
    "Finally, the more simulations I run, the closer I get to the true probability, which helps in making a more accurate guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "import random\n",
    "\n",
    "\n",
    "def prob_rain_more_than_n(p: Sequence[float], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    p: List of 365 daily rain probabilities.\n",
    "    n: Threshold for rainy days\n",
    "    num_sim: Number of simulations (10,000).\n",
    "\n",
    "    Returns:\n",
    "    Probability as a float (0 to 1). Multiplying by 100 for percentage.\n",
    "\n",
    "    \"\"\"\n",
    "    num_sim = 10000\n",
    "    count_greater_n = 0\n",
    "\n",
    "    #monte Carlo Sim\n",
    "    for _ in range(num_sim):\n",
    "        rainy_days = 0\n",
    "\n",
    "        for day in range(365):\n",
    "            if random.random() < p[day]:\n",
    "                rainy_days += 1\n",
    "        \n",
    "        if rainy_days > n:\n",
    "            count_greater_n += 1\n",
    "    \n",
    "    return count_greater_n / num_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.47% is the possibility of it raining atleast more than 100 days for the year\n"
     ]
    }
   ],
   "source": [
    "#Test case\n",
    "p = [0.3] * 365  # Test where each day has a 30% chance of rain\n",
    "n = 100\n",
    "probability = prob_rain_more_than_n(p, n)\n",
    "print(f\"{probability* 100:.2f}% is the possibility of it raining atleast more than {n} days for the year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2)  A phoneme is a sound unit (similar to a character for text). We have an extensive pronunciation\n",
    "dictionary (think millions of words). Below is a snippet:\n",
    "\n",
    "ABACUS AE B AH K AH S\n",
    "\n",
    "BOOK B UH K\n",
    "\n",
    "THEIR DH EH R\n",
    "\n",
    "THERE DH EH R\n",
    "\n",
    "TOMATO T AH M AA T OW\n",
    "\n",
    "TOMATO T AH M EY T OW\n",
    "\n",
    "Given a sequence of phonemes as input (e.g. [\"DH\", \"EH\", \"R\", \"DH\", \"EH\", \"R\"]), find all the combinations of the words that\n",
    "can produce this sequence (e.g. [[\"THEIR\", \"THEIR\"], [\"THEIR\", \"THERE\"], [\"THERE\", \"THEIR\"], [\"THERE\", \"THERE\"]]).\n",
    "\n",
    "You can preprocess the dictionary into a different data structure if needed.\n",
    "\n",
    "def find_word_combos_with_pronunciation(phonemes: Sequence[str]) -> Sequence[Sequence[str]]:\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions made: Preprocessing will lead to efficiency. \n",
    "Preprocessing/organizing the data will help find words that match the sequence of phonemes quickly. \n",
    "\n",
    "Part # 1:\n",
    "Process: The pronunciation dictionary is turned into a map where each key is a tuple of phonemes, and the value is a list of words that match those phonemes. \n",
    "\n",
    "Reason: Looking up words to match parts of the phoneme sequence can be done in an instant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part # 2:\n",
    "Process: Backtracking is like exploring a maze where you try different paths to see which one leads to the exit. \n",
    "In this case, the \"maze\" is our sequence of phonemes, and the \"exit\" is finding all the ways we can split this sequence into valid words. \n",
    "\n",
    "The process begins at the start of the phoneme sequence, attempting to match the longest possible prefix to a word. If a match is found, the remaining sequence is checked recursively for additional matches. If a path fails, backtracking occurs and tries an alternative path. This systematic approach ensures that all valid word combinations are considered. A preprocessed dictionary enhances efficiency, making the process significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, List, Dict, Tuple\n",
    "\n",
    "def preprocess_dictionary(pronunciation_dict: List[Tuple[str, List[str]]]):\n",
    "    \"\"\"\n",
    "    Preprocess the pronunciation dictionary into a mapping from phoneme sequences to words.\n",
    "    \"\"\"\n",
    "    phoneme_to_words = {}\n",
    "    for word, phonemes in pronunciation_dict:\n",
    "        phoneme_tuple = tuple(phonemes)\n",
    "        if phoneme_tuple not in phoneme_to_words:\n",
    "            phoneme_to_words[phoneme_tuple] = []\n",
    "        phoneme_to_words[phoneme_tuple].append(word)\n",
    "    return phoneme_to_words\n",
    "\n",
    "def find_word_combos_with_pronunciation(phonemes: Sequence[str], phoneme_to_words: Dict[Tuple[str, ...], List[str]]) -> Sequence[Sequence[str]]:\n",
    "    \"\"\"\n",
    "    Find all combinations of words that can produce the given sequence of phonemes.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def backtrack(start: int) -> List[List[str]]:\n",
    "        if start == len(phonemes):\n",
    "            return [[]]  # Base case: return a list with an empty list\n",
    "\n",
    "        results = []\n",
    "        for end in range(start + 1, len(phonemes) + 1):\n",
    "            current_phoneme_tuple = tuple(phonemes[start:end])\n",
    "            if current_phoneme_tuple in phoneme_to_words:\n",
    "                words = phoneme_to_words[current_phoneme_tuple]\n",
    "                for word in words:\n",
    "                    for rest in backtrack(end):\n",
    "                        results.append([word] + rest)\n",
    "        return results\n",
    "\n",
    "    return backtrack(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THEIR', 'THEIR'], ['THEIR', 'THERE'], ['THERE', 'THEIR'], ['THERE', 'THERE']]\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1:\n",
    "pronunciation_dict = [\n",
    "    (\"ABACUS\", [\"AE\", \"B\", \"AH\", \"K\", \"AH\", \"S\"]),\n",
    "    (\"BOOK\", [\"B\", \"UH\", \"K\"]),\n",
    "    (\"THEIR\", [\"DH\", \"EH\", \"R\"]),\n",
    "    (\"THERE\", [\"DH\", \"EH\", \"R\"]),\n",
    "    (\"TOMATO\", [\"T\", \"AH\", \"M\", \"AA\", \"T\", \"OW\"]),\n",
    "    (\"TOMATO\", [\"T\", \"AH\", \"M\", \"EY\", \"T\", \"OW\"])\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Assumption Made: the words produced as an output are always supposed to correspond to the \n",
    "order of  the phonemes sequence list/input.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "phoneme_to_words = preprocess_dictionary(pronunciation_dict)\n",
    "phonemes = [\"DH\", \"EH\", \"R\", \"DH\", \"EH\", \"R\"]\n",
    "combinations = find_word_combos_with_pronunciation(phonemes, phoneme_to_words)\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PEAR', 'BELL'], ['PAIR', 'BELL']]\n"
     ]
    }
   ],
   "source": [
    "# Test Case 2:\n",
    "pronunciation_dict = [\n",
    "    (\"APPLE\", [\"AE\", \"P\", \"AH\", \"L\"]),\n",
    "    (\"PEAR\", [\"P\", \"EH\", \"R\"]),\n",
    "    (\"BEAR\", [\"B\", \"EH\", \"R\"]),\n",
    "    (\"PAIR\", [\"P\", \"EH\", \"R\"]),\n",
    "    (\"PEEL\", [\"P\", \"IY\", \"L\"]),\n",
    "    (\"BELL\", [\"B\", \"EH\", \"L\"])\n",
    "]\n",
    "\n",
    "phoneme_to_words = preprocess_dictionary(pronunciation_dict)\n",
    "phonemes = [\"P\", \"EH\", \"R\", \"B\", \"EH\", \"L\"]\n",
    "phonemes_2 = [\"P\", \"EH\", \"R\",\"P\", \"EH\", \"R\"] #try_2\n",
    "combinations = find_word_combos_with_pronunciation(phonemes, phoneme_to_words)\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Implement CTC as this [paper](https://dl.acm.org/doi/abs/10.1145/1143844.1143891) describes. Your implementation should support both forward and\n",
    "backward propagation operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "In implementing the Connectionist Temporal Classification (CTC) algorithm, we assume that the input consists of sequences of log probabilities, which are used to maintain numerical stability during calculations. A special blank token is employed to facilitate the alignment of input sequences with target label sequences, allowing for the insertion of gaps between labels. The target label sequence is shorter than or equal to the input sequence length and does not initially contain blanks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert Blanks: The insert_blanks function modifies the target label sequence by inserting blank tokens between each label and at the start and end. \n",
    "This prepares the sequence for the forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "Args:\n",
    "\n",
    "labels: List of target labels (integers).\n",
    "blank: Index for the blank label (default is 0).\n",
    "Returns:\n",
    "\n",
    "list: A new list with blanks inserted.\n",
    "\n",
    "\"\"\"\n",
    "def insert_blanks(labels, blank=0):\n",
    "    new_labels = [blank]\n",
    "    for label in labels:\n",
    "        new_labels.extend([label, blank])\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation:\n",
    "\n",
    "The forward_pass function computes the forward variables using dynamic programming. This step calculates the probability of reaching each state at every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(log_probs, labels, blank=0):\n",
    "    \"\"\"\n",
    "    Compute the forward variables 'alpha' for CTC using dp.\n",
    "    \n",
    "    Args:\n",
    "        log_probs: Log probabilities of shape (T, num_classes).\n",
    "        labels: Target label sequence.\n",
    "        blank: Index for the blank label.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Forward variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    T, _ = log_probs.shape\n",
    "    ext_labels = insert_blanks(labels, blank)\n",
    "    S = len(ext_labels)\n",
    "    \n",
    "    alpha = np.full((T, S), -np.inf)\n",
    "    alpha[0, 0] = log_probs[0, blank]\n",
    "    if S > 1:\n",
    "        alpha[0, 1] = log_probs[0, ext_labels[1]]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        for s in range(S):\n",
    "            candidates = [alpha[t-1, s]]\n",
    "            if s > 0:\n",
    "                candidates.append(alpha[t-1, s-1])\n",
    "            if s > 1 and ext_labels[s] != blank and ext_labels[s] != ext_labels[s-2]:\n",
    "                candidates.append(alpha[t-1, s-2])\n",
    "            max_val = np.max(candidates)\n",
    "            if np.isfinite(max_val):\n",
    "                alpha[t, s] = max_val + np.log(np.sum(np.exp(np.array(candidates) - max_val)))\n",
    "            alpha[t, s] += log_probs[t, ext_labels[s]]\n",
    "    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Propagation: \n",
    "\n",
    "The backward_pass function computes the backward variables, representing the probability of completing the sequence from each state, also using dynamic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(log_probs, labels, blank=0):\n",
    "    \"\"\"\n",
    "    Compute the backward variables 'beta' for CTC using dp.\n",
    "    \n",
    "    Args:\n",
    "        log_probs: Log probabilities of shape (T, num_classes).\n",
    "        labels: Target label sequence.\n",
    "        blank: Index for the blank label.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Backward variables.\n",
    "    \"\"\"\n",
    "    T, _ = log_probs.shape\n",
    "    ext_labels = insert_blanks(labels, blank)\n",
    "    S = len(ext_labels)\n",
    "    \n",
    "    beta = np.full((T, S), -np.inf)\n",
    "    beta[T-1, S-1] = log_probs[T-1, blank]\n",
    "    if S > 1:\n",
    "        beta[T-1, S-2] = log_probs[T-1, ext_labels[S-2]]\n",
    "    \n",
    "    for t in range(T-2, -1, -1):\n",
    "        for s in range(S):\n",
    "            candidates = [beta[t+1, s]]\n",
    "            if s < S-1:\n",
    "                candidates.append(beta[t+1, s+1])\n",
    "            if s < S-2 and ext_labels[s] != blank and ext_labels[s] != ext_labels[s+2]:\n",
    "                candidates.append(beta[t+1, s+2])\n",
    "            max_val = np.max(candidates)\n",
    "            if np.isfinite(max_val):\n",
    "                beta[t, s] = max_val + np.log(np.sum(np.exp(np.array(candidates) - max_val)))\n",
    "            beta[t, s] += log_probs[t, ext_labels[s]]\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTC Loss and Gradient: The ctc_loss_and_grad function combines the forward and backward results to compute the total probability of the target sequence. \n",
    "\n",
    "The loss is the negative log of this probability, and the gradient is calculated for use in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_and_grad(log_probs, labels, blank=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the CTC loss and gradient with respect to the network outputs.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    log_probs: Log probabilities of shape (T, num_classes).\n",
    "    labels: Target label sequence.\n",
    "    blank: Index for the blank label.\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    float: CTC loss.\n",
    "    np.ndarray: Gradient of the loss with respect to log_probs.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    T, num_classes = log_probs.shape\n",
    "    alpha = forward_pass(log_probs, labels, blank)\n",
    "    beta = backward_pass(log_probs, labels, blank)\n",
    "    ext_labels = insert_blanks(labels, blank)\n",
    "    S = len(ext_labels)\n",
    "    \n",
    "    log_prob1 = alpha[T-1, S-1]\n",
    "    log_prob2 = alpha[T-1, S-2] if S > 1 else -np.inf\n",
    "    total_log_prob = np.logaddexp(log_prob1, log_prob2)\n",
    "    \n",
    "    if not np.isfinite(total_log_prob):\n",
    "        print(\"Warning: total_log_prob is not finite.\")\n",
    "    \n",
    "    loss = -total_log_prob\n",
    "    \n",
    "    grad = np.zeros_like(log_probs)\n",
    "    for t in range(T):\n",
    "        for s in range(S):\n",
    "            k = ext_labels[s]\n",
    "            grad[t, k] -= np.exp(alpha[t, s] + beta[t, s] - total_log_prob)\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yielding the CTC loss and gradient necessary for training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Loss: 69.05861912394417\n",
      "CTC Gradient shape: (50, 5)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    T = 50  # Number of time steps\n",
    "    num_classes = 5  # Number of classes including the blank\n",
    "    np.random.seed(42)\n",
    "    logits = np.random.randn(T, num_classes)\n",
    "    log_probs = np.log(np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True))\n",
    "    \n",
    "    target = [1, 2, 3]  # Example target label sequence\n",
    "    \n",
    "    loss, grad = ctc_loss_and_grad(log_probs, target, blank=0)\n",
    "    \n",
    "    print(\"CTC Loss:\", loss)\n",
    "    print(\"CTC Gradient shape:\", grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTC Loss: The value 69.05861912394417 represents the negative log likelihood of the target label sequence given the input sequence. \n",
    "In the context of training a neural network, this loss value would be used to update the model's weights to improve its predictions. \n",
    "A lower loss indicates a better alignment between the predicted and target sequences.\n",
    "\n",
    "Gradient Shape: The gradient shape (50, 5) indicates that the gradient has been computed for each time step (50 in total) and each class (5 classes, including the blank). \n",
    "This gradient is used to adjust the network's weights during backpropagation to minimize the CTC loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numerical Stability: Uses log-space calculations to prevent underflow or overflow, ensuring stability when working with probabilities.\n",
    "- Dynamic Programming: Utilizes dynamic programming in both forward and backward passes to efficiently compute alignment probabilities between input and target sequences.\n",
    "- Gradient Calculation: Computes gradients for each time step and class, enabling backpropagation to update network weights.\n",
    "- Integration: Designed for seamless integration into a neural network training pipeline, optimizing model parameters using CTC loss and gradients.\n",
    "\n",
    "the CTC handles unsegmented sequence data, such as speech, by aligning input sequences with target labels without requiring presegmented data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
